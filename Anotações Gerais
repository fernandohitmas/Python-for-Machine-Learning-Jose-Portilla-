Avaliação de modelo (Supervisionado e Binário)

 - Separação dos dados em: treino e teste
 - O conjunto de dados de treino é aquele utilizado para treinar o modelo de aprendizado de máquina e o conjunto de teste é utilizado para testar a qualidade do modelo.

Para avaliar um modelo deste tipo é comum utilizar a matriz de confusão

 - Acurácia: porcentagem de acertos dentro do número total de testes. É útil quando se trata de um conjunto de dados BALANCEADO.

 - No caso de conjuntos DESbalanceados: Recall e Precisão
 - RECALL: habilidade de um modelo de encontrar todos os casos relevantes dentro de um conjunto de dados. Seu cálculo é feito pela divisão de verdadeiros positivos pela soma de verdadeiros positivos e falsos negativos
 - PRECISÃO: habilidade de um modelo em identificar somente os pontos relevantes. É definido como: o número de verdadeiros positivos dividido pela soma de verdadeiros positivos e falsos positivos.
 - F1-SCORE: no caso de querer o balanço entre ambas as métricas. É calculada pela médias harmônica entre recall e precisão. Pune diferenças muito grandes entre ambas.

Pensando em aplicações reais (saúde), os modelos utilizados normalmente tentam avaliar de forma rápida se a pessoa está doente e se ela necessita de uma avaliação mais sensível e que comumente é mais invasiva. Faz sentido pensar que no caso de doenças é mais interessante focar em valores maiores de falsos positivos do que em falsos negativos, para que se tenha a maior cobertura possível das pessoas doentes.


Avaliação de modelos - Regressão

 - ERRO ABSOLUTO MÉDIO: média dos erros absolutos para cada ponto do conjunto de dados. Não pune pontos extremos.
 - ERRO QUADRÁTICO MÉDIO: média dos erros ao quadrado. 
 - RAIZ DO ERRO QUADRÁTICO MÉDIO: raiz quadrada de MSE. Mantém a mesma unidade do que está sendo avaliado


Dilema de Viés e verdadeiros
 - Overfitting e underfittin
 - ao se aumentar a complexidade de um modelo, ele naturalmente se ajustará aos dados de treinamento, porém quando o utilizamos os dados de teste ele tem uma performance muito ruim e com altos índices de erros. Logo, é dever do cientista de dados avaliar um modelo que nao esteja nos extremos e que consiga ponderar a complexidade e a métrica de avaliação do modelo.

Regressão Logística
 - é um método de CLASSIFICAÇÃO. Classificação de doenças, classificação de e-mail
 - A convenção é utilizar 0 e 1 

K Vizinhos mais Próximos
 - É um modelo de CLASSIFICAÇÃO
 - Utiliza os pontos mais próximos daquele que se deseja classificar para tomar a decisão.
 - O K é o parametro que indica quantos pontos serão utilizados nos cálculos do método
 - É muito simples
 - Funciona com qualquer quantidade de classes
 - Fácil de adicionar dados
 - Possui poucos parâmetros
 - Custo de predicao é muito alto
 - Não é muito bom para problemas de dimensao alta
 - Nao funciona muito bem com variáveis categóricas

Árvores de decisão e florestas aleatórias
 - método que envoler a estratificação ou a segmentação
 - São simples e de interpretação fácil
 - Não são compepetitivas quando contra os melhores modelos supervisionados
 - Tem aplicação tanto em regressão quanto em classificação
 - Critério de separação dos nós: Índice de Gini ou Entropia. Ambos são úteis na hora de indicar a pureza de um nó (se possui muitas observações de uma mesma classe)
 - Possuem variancia muito grande

Support Vector Machines (capitulo 9 de Intro to Statistical Learning)
 - SVMs - Supervisionado, reconhecimento de padrões
 - Tenta dividir o espaço das observações para categorizar novas observações. Utiliza um hiperplano que maximiza a margem que se separa dos dados
 - Os pontos que definem as margens são denominados como pontos de suporte, daí o nome de vetores de suporte
 - Truque de Kernel: adicionar uma dimensão.    

Grid Search
 - método de procura pela melhor combinação de parâmetros
 - pode ser custoso quando o conjunto aumenta

K Means Clustering - CLUSTERING
 - Separação dos dados em K clusters diferentes que possuem características parecidas
 - como escolher o número de clusters? uma forma é utilizar o SSE (sum of squared error) - técnica do cotovelo

 PCA (Análise de Componente Principal)
  - as componentes são linearmente transformadas de tal forma que a maior variancia é alinhada com o primeiro eixo, a segunda maior variância com o próximo eixo e por assim em diantes
  - é utilizado como ferramenta de visualização de dados
  - é utilizado para adicionar dados faltantes
  - REPRESENTAÇÃO DE MENOR DIMENSÃO, MAS QUE CONTÉM O MÁXIMO DE VARIAÇÃO POSSÍVEL DOS DADOS
  - A primeira componente do PCA também poder ser interpretado como a dimensão que os dados estão mais próximos

Sistemas de Recomendação
 - Content Base - de acordo com as semelhanças entre os itens
 - Collaborative Filtering - de acordo com o comportamento do usuario
