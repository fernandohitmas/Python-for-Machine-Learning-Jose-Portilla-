Avaliação de modelo (Supervisionado e Binário)

 - Separação dos dados em: treino e teste
 - O conjunto de dados de treino é aquele utilizado para treinar o modelo de aprendizado de máquina e o conjunto de teste é utilizado para testar a qualidade do modelo.

Para avaliar um modelo deste tipo é comum utilizar a matriz de confusão

 - Acurácia: porcentagem de acertos dentro do número total de testes. É útil quando se trata de um conjunto de dados BALANCEADO.

 - No caso de conjuntos DESbalanceados: Recall e Precisão
 - RECALL: habilidade de um modelo de encontrar todos os casos relevantes dentro de um conjunto de dados. Seu cálculo é feito pela divisão de verdadeiros positivos pela soma de verdadeiros positivos e falsos negativos
 - PRECISÃO: habilidade de um modelo em identificar somente os pontos relevantes. É definido como: o número de verdadeiros positivos dividido pela soma de verdadeiros positivos e falsos positivos.
 - F1-SCORE: no caso de querer o balanço entre ambas as métricas. É calculada pela médias harmônica entre recall e precisão. Pune diferenças muito grandes entre ambas.

Pensando em aplicações reais (saúde), os modelos utilizados normalmente tentam avaliar de forma rápida se a pessoa está doente e se ela necessita de uma avaliação mais sensível e que comumente é mais invasiva. Faz sentido pensar que no caso de doenças é mais interessante focar em valores maiores de falsos positivos do que em falsos negativos, para que se tenha a maior cobertura possível das pessoas doentes.


Avaliação de modelos - Regressão

 - ERRO ABSOLUTO MÉDIO: média dos erros absolutos para cada ponto do conjunto de dados. Não pune pontos extremos.
 - ERRO QUADRÁTICO MÉDIO: média dos erros ao quadrado. 
 - RAIZ DO ERRO QUADRÁTICO MÉDIO: raiz quadrada de MSE. Mantém a mesma unidade do que está sendo avaliado


Dilema de Viés e verdadeiros
 - Overfitting e underfittin
 - ao se aumentar a complexidade de um modelo, ele naturalmente se ajustará aos dados de treinamento, porém quando o utilizamos os dados de teste ele tem uma performance muito ruim e com altos índices de erros. Logo, é dever do cientista de dados avaliar um modelo que nao esteja nos extremos e que consiga ponderar a complexidade e a métrica de avaliação do modelo.

Regressão Logística
 - é um método de CLASSIFICAÇÃO. Classificação de doenças, classificação de e-mail
 - A convenção é utilizar 0 e 1 

K Vizinhos mais Próximos
 - É um modelo de CLASSIFICAÇÃO
 - Utiliza os pontos mais próximos daquele que se deseja classificar para tomar a decisão.
 - O K é o parametro que indica quantos pontos serão utilizados nos cálculos do método
 - É muito simples
 - Funciona com qualquer quantidade de classes
 - Fácil de adicionar dados
 - Possui poucos parâmetros
 - Custo de predicao é muito alto
 - Não é muito bom para problemas de dimensao alta
 - Nao funciona muito bem com variáveis categóricas

Árvores de decisão e florestas aleatórias
 - método que envoler a estratificação ou a segmentação
 - São simples e de interpretação fácil
 - Não são compepetitivas quando contra os melhores modelos supervisionados
 - Tem aplicação tanto em regressão quanto em classificação
 - Critério de separação dos nós: Índice de Gini ou Entropia. Ambos são úteis na hora de indicar a pureza de um nó (se possui muitas observações de uma mesma classe)
 - Possuem variancia muito grande

Support Vector Machines (capitulo 9 de Intro to Statistical Learning)
 - SVMs - Supervisionado, reconhecimento de padrões
 - Tenta dividir o espaço das observações para categorizar novas observações. Utiliza um hiperplano que maximiza a margem que se separa dos dados
 - Os pontos que definem as margens são denominados como pontos de suporte, daí o nome de vetores de suporte
 - Truque de Kernel: adicionar uma dimensão.    

Grid Search
 - método de procura pela melhor combinação de parâmetros
 - pode ser custoso quando o conjunto aumenta

K Means Clustering - CLUSTERING
 - Separação dos dados em K clusters diferentes que possuem características parecidas
 - como escolher o número de clusters? uma forma é utilizar o SSE (sum of squared error) - técnica do cotovelo

 PCA (Análise de Componente Principal)
  - as componentes são linearmente transformadas de tal forma que a maior variancia é alinhada com o primeiro eixo, a segunda maior variância com o próximo eixo e por assim em diantes
  - é utilizado como ferramenta de visualização de dados
  - é utilizado para adicionar dados faltantes
  - REPRESENTAÇÃO DE MENOR DIMENSÃO, MAS QUE CONTÉM O MÁXIMO DE VARIAÇÃO POSSÍVEL DOS DADOS
  - A primeira componente do PCA também poder ser interpretado como a dimensão que os dados estão mais próximos

Sistemas de Recomendação
 - Content Base - de acordo com as semelhanças entre os itens
 - Collaborative Filtering - de acordo com o comportamento do usuario

Processamento de Linguagem Natural
 - Processamento de aquivos de textos com o intuito de se mapear as su as similaridades, podendo, dessa forma, fazer a sua categorização.
 - Corpus: conjunto de documentos a serem analisados
 - Bag of Words: documento contendo palavras
 - Term Frequency: a importância que os termo tem dentro do documento - Número de ocorrências do termo T no documento D
 - Inverse Document Frequency: Importancia que o termo tem dentro do Corpus - IDF = log(D/T), onde D é o número total de documentos e T é o número de documentos que tem o termo
 - TF-IDF = TF_xy*log(N/df_x), N é o número total de documentoes, tf_xy é a frequência de x em y e df_x é o número de documentos que contém x

Redes Neurais e Deep Learning
  - Tópicos: Modelo de Perceptron, funções de ativção, funções de custo, feed foward rede neural, back progation
  - Redes neurais tentam imitar o funcionamento de uma rede neural biológica (cérebro)
  - Modelo Perceptron: como funcionam neurônios. Dendritos > Núcleos > Axônios. inputs > f(x) > y >output
  - Idealmente, o modelo deveria ter algum modo de aprender. Uma forma de fazer isso é utilizar um vetor de pesos que multiplicam o vetor de entradas de f(X). Assim, é possível modificar esses pesos para chegar no resultado almejado.
  - Além do peso também utiliza-se um "viés" na função. w1*x1 + b, por exemplo.
  - perceptron: y_hat = sum xi*wi + bi
  - O modelo de perceptron único pode ser expandido em um modelo de multicamadas de perceptrons.
  - camada1(p1,p2,p3) > camada2(q1,q2) > camada3(c1,c2,c3)
  - quando todos os perceptrons estão ligados com a próxima camada é chamada de CAMADA CHEIA/TOTALMENTE LIGADA.
  - As informações seguem apenas uma direção, feed foward.
  - primeira camada é chamada de input layer e a última é chamada de output layer e qualquer um entre elas é chamada de hidden layers
  - Hidden Layers tem a intepretabilidade bastante comprometida devido a distância que estão dos inputs e dos outputs
  - Uma rede neural se torna um rede profunda quando contém 2 ou mais hidden layers
  - Redes Neurais podem aproximar qualquer função (universal approximation theorem)
  - importante considerar limites dentro dos quais a função f(x) vai atuar e como as funções de ativação funcionam 
  - a curva logística é bastante utilizada como função de ativação 
  - outras funções: tangente hiperbólica, Rectified Linear Unit(ReLu, max(0,z))
  - organizar as saídas em Múltiplas classes > one hot encoding 
  - sofmax function
  - funções exclusivas
  - Minimizar a função de custo utilizando o método de gradiente descendente ADAPTATIVO (adam)
  - para problemas de classficação, normalmente é utilizado cross entropy loss function. Assume que tem uma distribuição de probabilidade para cada categoria
  - BACKPROPAGATION 

Tensorflow vs Keras:
  -  Keras é uma biblioteca de python que permite utilizar outras bibliotecas de aprendizado profundo, como o tensorflow. 
  - Early Stopping: o keras para automaticamente o treinamento baseado numa condição da função de perda durante o período de validação dos dados, passado no método .fit do modelo

Big Data:
  - Local vs Distribuido: local usa todos os cores da máquina, mas distribuido utiliza menos cores de mais máquinas conectadas por alguma rede.
  - Distribuido: vantagem de ser escalável, possuem "Fault Tolerance"
  - Arquitetura utilizando Hadoop: é uma forma de distribuir grandes arquivos através de múltiplas máquinas (Hadoop Distributed File System / HDFS). Permite que o usuário trabalhe com grandes conjuntos de dados, permite blocos duplicados de dados, utiliza MapReduce 
  - HDFS: um nó principal que controla outros. A multiplicidade de nós permite que dados sejam duplicados, permite paralelização e também prevenção de problemas. Baixo custo
  - MapReduce é uma forma de computação distribuida para formatos de dados como HDFS e consiste num Job Tracker e mútiplos Task Trackers

Spark:
  - Projeto do Apache
  - Aceita dados guardados de diversas formas, tais como Cassandra, AWS S3, HDFS e mais
  - Spark pode ser até 100x mais rápido que MapReduce
  - MapReduce usa o disco e Spark usa RAM
  - A ideia principal do Spark vem de RDD, ou, Resilient Distributed Dataset. RDD tem 4 features principais: Coleção Distribudída de Dados, tolerância a problemas, operações paralelizadas/particionadas e a habilidade de usar diversas fontes de dados
  - RDDs são imutáveis, lazily evaluated e cacheable
  - Existem 2 tipos de operações RDD, transformações e Ações
  - Ações Básicas: First (Retorna o primeiro elemento do RDD), Collect (retorna todos os elementos do RDD como um array no programa principal), Count (retorna o número de eelementos do RDD), Take (retorna um array com os primeiros n elementos do RDD)
  - Transformações Básicas: Filter (aplica uma função em cada elemento e retorna aqueles que forem verdadeiros), Map (transforma cada elemento e preserva o número de elementos), FlatMap (transforma cada elemento em um 0xN elemento e provavelmente muda o número de elementos)
  - Frequentemente RDDs armazerão os dados na forma de tuplas (chave, valor)
  - Reduce: uma ação que agregará os elementos da RDD utilizando uma função que retorna apenas um elemento
  - ReduceByKey:ma ação que agregará os elementos da RDD utilizando uma função que retorna apenas um par de elementos
  - Ambos reduce são similar a group by

AWS - EC2:
  - Amazon Elastic Compute Cloud (EC2): computação em nuvem que sob demanda. "Computador virtual que é acessado via internet"
  - 1. Criar uma instância EC2 na AWS
  - 2. Usar uma chave SSH para conectar ao EC2 pela internet
  - Criando uma instância:
  - 1. Login nos serviços AWS
  - 2. EC2
  - 3. Executar uma instância
  - 4. Escolher uma AMI - Amazon Machine Image ou OS da máquina virtual. No caso foi escolhido um ubuntu de nível gratuito
  - 5. Escolher tipo de instância: t2.micro (1vCPU e 1 Gib de memória)
  - 6. Par de Chaves: key: myspark2
  - 7. Configurar Rede: default
  - 8. Configurar armazenamento default
  - OBSERVAÇÃO: estado da instância > encerrar instância