Avaliação de modelo (Supervisionado e Binário)

 - Separação dos dados em: treino e teste
 - O conjunto de dados de treino é aquele utilizado para treinar o modelo de aprendizado de máquina e o conjunto de teste é utilizado para testar a qualidade do modelo.

Para avaliar um modelo deste tipo é comum utilizar a matriz de confusão

 - Acurácia: porcentagem de acertos dentro do número total de testes. É útil quando se trata de um conjunto de dados BALANCEADO.

 - No caso de conjuntos DESbalanceados: Recall e Precisão
 - RECALL: habilidade de um modelo de encontrar todos os casos relevantes dentro de um conjunto de dados. Seu cálculo é feito pela divisão de verdadeiros positivos pela soma de verdadeiros positivos e falsos negativos
 - PRECISÃO: habilidade de um modelo em identificar somente os pontos relevantes. É definido como: o número de verdadeiros positivos dividido pela soma de verdadeiros positivos e falsos positivos.
 - F1-SCORE: no caso de querer o balanço entre ambas as métricas. É calculada pela médias harmônica entre recall e precisão. Pune diferenças muito grandes entre ambas.

Pensando em aplicações reais (saúde), os modelos utilizados normalmente tentam avaliar de forma rápida se a pessoa está doente e se ela necessita de uma avaliação mais sensível e que comumente é mais invasiva. Faz sentido pensar que no caso de doenças é mais interessante focar em valores maiores de falsos positivos do que em falsos negativos, para que se tenha a maior cobertura possível das pessoas doentes.


Avaliação de modelos - Regressão

 - ERRO ABSOLUTO MÉDIO: média dos erros absolutos para cada ponto do conjunto de dados. Não pune pontos extremos.
 - ERRO QUADRÁTICO MÉDIO: média dos erros ao quadrado. 
 - RAIZ DO ERRO QUADRÁTICO MÉDIO: raiz quadrada de MSE. Mantém a mesma unidade do que está sendo avaliado


Dilema de Viés e verdadeiros
 - Overfitting e underfittin
 - ao se aumentar a complexidade de um modelo, ele naturalmente se ajustará aos dados de treinamento, porém quando o utilizamos os dados de teste ele tem uma performance muito ruim e com altos índices de erros. Logo, é dever do cientista de dados avaliar um modelo que nao esteja nos extremos e que consiga ponderar a complexidade e a métrica de avaliação do modelo.

Regressão Logística
 - é um método de CLASSIFICAÇÃO. Classificação de doenças, classificação de e-mail
 - A convenção é utilizar 0 e 1 

K Vizinhos mais Próximos
 - É um modelo de CLASSIFICAÇÃO
 - Utiliza os pontos mais próximos daquele que se deseja classificar para tomar a decisão.
 - O K é o parametro que indica quantos pontos serão utilizados nos cálculos do método
 - É muito simples
 - Funciona com qualquer quantidade de classes
 - Fácil de adicionar dados
 - Possui poucos parâmetros
 - Custo de predicao é muito alto
 - Não é muito bom para problemas de dimensao alta
 - Nao funciona muito bem com variáveis categóricas

Árvores de decisão e florestas aleatórias
 - método que envoler a estratificação ou a segmentação
 - São simples e de interpretação fácil
 - Não são compepetitivas quando contra os melhores modelos supervisionados
 - Tem aplicação tanto em regressão quanto em classificação
 - Critério de separação dos nós: Índice de Gini ou Entropia. Ambos são úteis na hora de indicar a pureza de um nó (se possui muitas observações de uma mesma classe)
 - Possuem variancia muito grande

Support Vector Machines (capitulo 9 de Intro to Statistical Learning)
 - SVMs - Supervisionado, reconhecimento de padrões
 - Tenta dividir o espaço das observações para categorizar novas observações. Utiliza um hiperplano que maximiza a margem que se separa dos dados
 - Os pontos que definem as margens são denominados como pontos de suporte, daí o nome de vetores de suporte
 - Truque de Kernel: adicionar uma dimensão.    

Grid Search
 - método de procura pela melhor combinação de parâmetros
 - pode ser custoso quando o conjunto aumenta

K Means Clustering - CLUSTERING
 - Separação dos dados em K clusters diferentes que possuem características parecidas
 - como escolher o número de clusters? uma forma é utilizar o SSE (sum of squared error) - técnica do cotovelo

 PCA (Análise de Componente Principal)
  - as componentes são linearmente transformadas de tal forma que a maior variancia é alinhada com o primeiro eixo, a segunda maior variância com o próximo eixo e por assim em diantes
  - é utilizado como ferramenta de visualização de dados
  - é utilizado para adicionar dados faltantes
  - REPRESENTAÇÃO DE MENOR DIMENSÃO, MAS QUE CONTÉM O MÁXIMO DE VARIAÇÃO POSSÍVEL DOS DADOS
  - A primeira componente do PCA também poder ser interpretado como a dimensão que os dados estão mais próximos

Sistemas de Recomendação
 - Content Base - de acordo com as semelhanças entre os itens
 - Collaborative Filtering - de acordo com o comportamento do usuario

Processamento de Linguagem Natural
 - Processamento de aquivos de textos com o intuito de se mapear as su as similaridades, podendo, dessa forma, fazer a sua categorização.
 - Corpus: conjunto de documentos a serem analisados
 - Bag of Words: documento contendo palavras
 - Term Frequency: a importância que os termo tem dentro do documento - Número de ocorrências do termo T no documento D
 - Inverse Document Frequency: Importancia que o termo tem dentro do Corpus - IDF = log(D/T), onde D é o número total de documentos e T é o número de documentos que tem o termo
 - TF-IDF = TF_xy*log(N/df_x), N é o número total de documentoes, tf_xy é a frequência de x em y e df_x é o número de documentos que contém x

Redes Neurais e Deep Learning
  - Tópicos: Modelo de Perceptron, funções de ativção, funções de custo, feed foward rede neural, back progation
  - Redes neurais tentam imitar o funcionamento de uma rede neural biológica (cérebro)
  - Modelo Perceptron: como funcionam neurônios. Dendritos > Núcleos > Axônios. inputs > f(x) > y >output
  - Idealmente, o modelo deveria ter algum modo de aprender. Uma forma de fazer isso é utilizar um vetor de pesos que multiplicam o vetor de entradas de f(X). Assim, é possível modificar esses pesos para chegar no resultado almejado.
  - Além do peso também utiliza-se um "viés" na função. w1*x1 + b, por exemplo.
  - perceptron: y_hat = sum xi*wi + bi
  - O modelo de perceptron único pode ser expandido em um modelo de multicamadas de perceptrons.
  - camada1(p1,p2,p3) > camada2(q1,q2) > camada3(c1,c2,c3)
  - quando todos os perceptrons estão ligados com a próxima camada é chamada de CAMADA CHEIA/TOTALMENTE LIGADA.
  - As informações seguem apenas uma direção, feed foward.
  - primeira camada é chamada de input layer e a última é chamada de output layer e qualquer um entre elas é chamada de hidden layers
  - Hidden Layers tem a intepretabilidade bastante comprometida devido a distância que estão dos inputs e dos outputs
  - Uma rede neural se torna um rede profunda quando contém 2 ou mais hidden layers
  - Redes Neurais podem aproximar qualquer função (universal approximation theorem)
  - importante considerar limites dentro dos quais a função f(x) vai atuar e como as funções de ativação funcionam 
  - a curva logística é bastante utilizada como função de ativação 
  - outras funções: tangente hiperbólica, Rectified Linear Unit(ReLu, max(0,z))
  - organizar as saídas em Múltiplas classes > one hot encoding 
  - sofmax function
  - funções exclusivas
  - Minimizar a função de custo utilizando o método de gradiente descendente ADAPTATIVO (adam)
  - para problemas de classficação, normalmente é utilizado cross entropy loss function. Assume que tem uma distribuição de probabilidade para cada categoria
  - BACKPROPAGATION 

Tensorflow vs Keras:
  -  Keras é uma biblioteca de python que permite utilizar outras bibliotecas de aprendizado profundo, como o tensorflow. 
  - Early Stopping: o keras para automaticamente o treinamento baseado numa condição da função de perda durante o período de validação dos dados, passado no método .fit do modelo

Big Data:
  - Local vs Distribuido: local usa todos os cores da máquina, mas distribuido utiliza menos cores de mais máquinas conectadas por alguma rede.
  - Distribuido: vantagem de ser escalável, possuem "Fault Tolerance"
  - Arquitetura utilizando Hadoop: é uma forma de distribuir grandes arquivos através de múltiplas máquinas (Hadoop Distributed File System / HDFS). Permite que o usuário trabalhe com grandes conjuntos de dados, permite blocos duplicados de dados, utiliza MapReduce 
  - HDFS: um nó principal que controla outros. A multiplicidade de nós permite que dados sejam duplicados, permite paralelização e também prevenção de problemas. Baixo custo
  - MapReduce é uma forma de computação distribuida para formatos de dados como HDFS e consiste num Job Tracker e mútiplos Task Trackers

Spark:
  - Projeto do Apache
  - Aceita dados guardados de diversas formas, tais como Cassandra, AWS S3, HDFS e mais
  - Spark pode ser até 100x mais rápido que MapReduce
  - MapReduce usa o disco e Spark usa RAM
  - A ideia principal do Spark vem de RDD, ou, Resilient Distributed Dataset. RDD tem 4 features principais: Coleção Distribudída de Dados, tolerância a problemas, operações paralelizadas/particionadas e a habilidade de usar diversas fontes de dados
  - RDDs são imutáveis, lazily evaluated e cacheable
  - Existem 2 tipos de operações RDD, transformações e Ações
  - Ações Básicas: First (Retorna o primeiro elemento do RDD), Collect (retorna todos os elementos do RDD como um array no programa principal), Count (retorna o número de eelementos do RDD), Take (retorna um array com os primeiros n elementos do RDD)
  - Transformações Básicas: Filter (aplica uma função em cada elemento e retorna aqueles que forem verdadeiros), Map (transforma cada elemento e preserva o número de elementos), FlatMap (transforma cada elemento em um 0xN elemento e provavelmente muda o número de elementos)
  - Frequentemente RDDs armazerão os dados na forma de tuplas (chave, valor)
  - Reduce: uma ação que agregará os elementos da RDD utilizando uma função que retorna apenas um elemento
  - ReduceByKey:ma ação que agregará os elementos da RDD utilizando uma função que retorna apenas um par de elementos
  - Ambos reduce são similar a group by

AWS - EC2:
  - Amazon Elastic Compute Cloud (EC2): computação em nuvem que sob demanda. "Computador virtual que é acessado via internet"
  - 1. Criar uma instância EC2 na AWS
  - 2. Usar uma chave SSH para conectar ao EC2 pela internet
  - Criando uma instância:
  - 1. Login nos serviços AWS
  - 2. EC2
  - 3. Executar uma instância
  - 4. Escolher uma AMI - Amazon Machine Image ou OS da máquina virtual. No caso foi escolhido um ubuntu de nível gratuito
  - 5. Escolher tipo de instância: t2.micro (1vCPU e 1 Gib de memória)
  - 6. Par de Chaves: sempre usar nova para instância nova
  - 7. Configurar Rede: default
  - 8. Configurar armazenamento default
  - OBSERVAÇÃO: estado da instância > encerrar instância

Pyspark SetUp:

Recomendação  de Ajay no QA do curso:

First, you needed to clean everything from your virtual machine, by running this command:

$ rm -rfv / home/ubuntu /{*,.*}

Getting Anaconda:

$ wget http://repo.continuum.io/archive/Anaconda3-2020.02-Linux-x86_64.sh

$ bash Anaconda3-2020.02-Linux-x86_64.sh

then run:

$ source ~/.bashrc

This will change your environment to Conda's (Base)

(base) ubuntu@ip-172-31-16-113

now we will install everything in this environment only:



let's check the version of Python:

$ which python

/home/ubuntu/anaconda3/bin/python

$ python --version

it will give you -- Python 3.7.6

Now run below codes for configuration of Jupyter Notebook and connection with EC2

$ jupyter notebook --generate-config

$ mkdir certs

$ cd certs

$ sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem

$ sudo chmod 777 mycert.pem

$ cd ~/.jupyter/

$ vi jupyter_notebook_config.py

press "i" and the paste this in file:

  # Configuration file for jupyter-notebook.

  c = get_config()

  # Notebook config this is where you saved your pem cert

  c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem'

  # Listen on all IPs

  c.NotebookApp.ip = '0.0.0.0'

  # Allow all origins

  c.NotebookApp.allow_origin = '*'

  # Do not open browser by default

  c.NotebookApp.open_browser = False

  # Fix port to 8888

  c.NotebookApp.port = 8888

--  now press escape and then wq!

Now come out to main home directory and run jupyter Notebook command:

$ cd

$ jupyter notebook

>>>> IMPORTANTE: entrar na instância e adicionar porta 8888 no grupo de segurança.
 - 1. Instância EC2 na AWS
 - 2. Selecionar a instância de interesse
 - Scrollar para baixo e selecionar a aba de Segurança
 - Clicar no grupo de segurança
 - Scrollar para baixo e clicar em Editar Regras de Entrada
 - Adicionar: Tipo > TCP Personalizado, Intervalo de Portas > 8888, Origem > Personalizado (0.0.0.0/0)

Now go to the browser and for me, Chrome didn't work and didn't let me pass through the security warning so I use Safari which gives me a lame warning before letting me in:

copy your DNS from Ec2 and paste it as mentioned below:

https://DNS:8888

DNS  = ec2-52-15-252-200.us-east-2.compute.amazonaws.com

so actually you will be pasting in browser(Safari):

https://ec2-52-15-252-200.us-east-2.compute.amazonaws.com:8888

Due to security reasons jupyter notebook will ask you token, which you can get easily from your terminal:

[I 21:52:47.706 NotebookApp] Serving notebooks from local directory: /home/ubuntu

[I 21:52:47.707 NotebookApp] The Jupyter Notebook is running at:

[I 21:52:47.707 NotebookApp] https://ip-172-31-16-113:8888/?token=675a256b95cd7ae813217424243deeed4908f8b46ea1a252

[I 21:52:47.707 NotebookApp] or https://127.0.0.1:8888/?token=675a256b95cd7ae813217424243deeed4908f8b46ea1a252

(Don't worry I change some of the values so you won't be able to access my virtual machine ;)

so copy this token and paste it in Jupyter Notebook and it will let you in!

Now press CTRL+C to end your session for further installations:



Installing Java and Scala:

$ sudo apt-get update

$ sudo apt-get install default-jre

$ java -version

$ sudo apt-get install scala

$ scala –version



Now install PIP:

export PATH=$PATH:$HOME/anaconda3/bin

conda install pip

IMP: Execution will ask you to upgrade Conda but please don't upgrade by simply typing No when prompter will ask you.

now install:

$ pip install py4j

$ now install Hadoop and Spark:

$ wget https://ftp.nluug.nl/internet/apache/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz

$ sudo tar -zxvf spark-3.0.1-bin-hadoop2.7.tgz



$ export SPARK_HOME='/home/ubuntu/spark-3.0.1-bin-hadoop2.7'

$ export PATH=$SPARK_HOME:$PATH

$ export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

$ jupyter notebook



Now again open Jupyter Notebook:

$ Jupyter Notebook



from pyspark import SparkContext

sc = SparkContext()

I hope this will work smoothly for you !! all the best!