Avaliação de modelo (Supervisionado e Binário)

 - Separação dos dados em: treino e teste
 - O conjunto de dados de treino é aquele utilizado para treinar o modelo de aprendizado de máquina e o conjunto de teste é utilizado para testar a qualidade do modelo.

Para avaliar um modelo deste tipo é comum utilizar a matriz de confusão

 - Acurácia: porcentagem de acertos dentro do número total de testes. É útil quando se trata de um conjunto de dados BALANCEADO.

 - No caso de conjuntos DESbalanceados: Recall e Precisão
 - RECALL: habilidade de um modelo de encontrar todos os casos relevantes dentro de um conjunto de dados. Seu cálculo é feito pela divisão de verdadeiros positivos pela soma de verdadeiros positivos e falsos negativos
 - PRECISÃO: habilidade de um modelo em identificar somente os pontos relevantes. É definido como: o número de verdadeiros positivos dividido pela soma de verdadeiros positivos e falsos positivos.
 - F1-SCORE: no caso de querer o balanço entre ambas as métricas. É calculada pela médias harmônica entre recall e precisão. Pune diferenças muito grandes entre ambas.

Pensando em aplicações reais (saúde), os modelos utilizados normalmente tentam avaliar de forma rápida se a pessoa está doente e se ela necessita de uma avaliação mais sensível e que comumente é mais invasiva. Faz sentido pensar que no caso de doenças é mais interessante focar em valores maiores de falsos positivos do que em falsos negativos, para que se tenha a maior cobertura possível das pessoas doentes.


Avaliação de modelos - Regressão

 - ERRO ABSOLUTO MÉDIO: média dos erros absolutos para cada ponto do conjunto de dados. Não pune pontos extremos.
 - ERRO QUADRÁTICO MÉDIO: média dos erros ao quadrado. 
 - RAIZ DO ERRO QUADRÁTICO MÉDIO: raiz quadrada de MSE. Mantém a mesma unidade do que está sendo avaliado


Dilema de Viés e verdadeiros
 - Overfitting e underfittin
 - ao se aumentar a complexidade de um modelo, ele naturalmente se ajustará aos dados de treinamento, porém quando o utilizamos os dados de teste ele tem uma performance muito ruim e com altos índices de erros. Logo, é dever do cientista de dados avaliar um modelo que nao esteja nos extremos e que consiga ponderar a complexidade e a métrica de avaliação do modelo.

Regressão Logística
 - é um método de CLASSIFICAÇÃO. Classificação de doenças, classificação de e-mail
 - A convenção é utilizar 0 e 1 

K Vizinhos mais Próximos
 - É um modelo de CLASSIFICAÇÃO
 - Utiliza os pontos mais próximos daquele que se deseja classificar para tomar a decisão.
 - O K é o parametro que indica quantos pontos serão utilizados nos cálculos do método
 - É muito simples
 - Funciona com qualquer quantidade de classes
 - Fácil de adicionar dados
 - Possui poucos parâmetros
 - Custo de predicao é muito alto
 - Não é muito bom para problemas de dimensao alta
 - Nao funciona muito bem com variáveis categóricas

Árvores de decisão e florestas aleatórias
 - método que envoler a estratificação ou a segmentação
 - São simples e de interpretação fácil
 - Não são compepetitivas quando contra os melhores modelos supervisionados
 - Tem aplicação tanto em regressão quanto em classificação
 - Critério de separação dos nós: Índice de Gini ou Entropia. Ambos são úteis na hora de indicar a pureza de um nó (se possui muitas observações de uma mesma classe)
 - Possuem variancia muito grande

Support Vector Machines (capitulo 9 de Intro to Statistical Learning)
 - SVMs - Supervisionado, reconhecimento de padrões
 - Tenta dividir o espaço das observações para categorizar novas observações. Utiliza um hiperplano que maximiza a margem que se separa dos dados
 - Os pontos que definem as margens são denominados como pontos de suporte, daí o nome de vetores de suporte
 - Truque de Kernel: adicionar uma dimensão.    

Grid Search
 - método de procura pela melhor combinação de parâmetros
 - pode ser custoso quando o conjunto aumenta

K Means Clustering - CLUSTERING
 - Separação dos dados em K clusters diferentes que possuem características parecidas
 - como escolher o número de clusters? uma forma é utilizar o SSE (sum of squared error) - técnica do cotovelo

 PCA (Análise de Componente Principal)
  - as componentes são linearmente transformadas de tal forma que a maior variancia é alinhada com o primeiro eixo, a segunda maior variância com o próximo eixo e por assim em diantes
  - é utilizado como ferramenta de visualização de dados
  - é utilizado para adicionar dados faltantes
  - REPRESENTAÇÃO DE MENOR DIMENSÃO, MAS QUE CONTÉM O MÁXIMO DE VARIAÇÃO POSSÍVEL DOS DADOS
  - A primeira componente do PCA também poder ser interpretado como a dimensão que os dados estão mais próximos

Sistemas de Recomendação
 - Content Base - de acordo com as semelhanças entre os itens
 - Collaborative Filtering - de acordo com o comportamento do usuario

Processamento de Linguagem Natural
 - Processamento de aquivos de textos com o intuito de se mapear as su as similaridades, podendo, dessa forma, fazer a sua categorização.
 - Corpus: conjunto de documentos a serem analisados
 - Bag of Words: documento contendo palavras
 - Term Frequency: a importância que os termo tem dentro do documento - Número de ocorrências do termo T no documento D
 - Inverse Document Frequency: Importancia que o termo tem dentro do Corpus - IDF = log(D/T), onde D é o número total de documentos e T é o número de documentos que tem o termo
 - TF-IDF = TF_xy*log(N/df_x), N é o número total de documentoes, tf_xy é a frequência de x em y e df_x é o número de documentos que contém x

Redes Neurais e Deep Learning
  - Tópicos: Modelo de Perceptron, funções de ativção, funções de custo, feed foward rede neural, back progation
  - Redes neurais tentam imitar o funcionamento de uma rede neural biológica (cérebro)
  - Modelo Perceptron: como funcionam neurônios. Dendritos > Núcleos > Axônios. inputs > f(x) > y >output
  - Idealmente, o modelo deveria ter algum modo de aprender. Uma forma de fazer isso é utilizar um vetor de pesos que multiplicam o vetor de entradas de f(X). Assim, é possível modificar esses pesos para chegar no resultado almejado.
  - Além do peso também utiliza-se um "viés" na função. w1*x1 + b, por exemplo.
  - perceptron: y_hat = sum xi*wi + bi
  - O modelo de perceptron único pode ser expandido em um modelo de multicamadas de perceptrons.
  - camada1(p1,p2,p3) > camada2(q1,q2) > camada3(c1,c2,c3)
  - quando todos os perceptrons estão ligados com a próxima camada é chamada de CAMADA CHEIA/TOTALMENTE LIGADA.
  - As informações seguem apenas uma direção, feed foward.
  - primeira camada é chamada de input layer e a última é chamada de output layer e qualquer um entre elas é chamada de hidden layers
  - Hidden Layers tem a intepretabilidade bastante comprometida devido a distância que estão dos inputs e dos outputs
  - Uma rede neural se torna um rede profunda quando contém 2 ou mais hidden layers
  - Redes Neurais podem aproximar qualquer função (universal approximation theorem)
  - importante considerar limites dentro dos quais a função f(x) vai atuar e como as funções de ativação funcionam
  - a curva logística é bastante utilizada como função de ativação 
  - outras funções: tangente hiperbólica, Rectified Linear Unit(ReLu, max(0,z))
  - organizar as saídas em Múltiplas classes > one hot encoding 
  - sofmax function
  - funções exclusivas
  - Minimizar a função de custo utilizando o método de gradiente descendente ADAPTATIVO (adam)
  - para problemas de classficação, normalmente é utilizado cross entropy loss function. Assume que tem uma distribuição de probabilidade para cada categoria
  - BACKPROPAGATION 